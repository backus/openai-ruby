#!/usr/bin/env ruby

require 'bundler/setup'
require 'openai'
require 'dotenv'
require 'pry'
require 'pry-byebug'
require 'slop'
require 'yaml'

Dotenv.load
api = OpenAI.new(ENV.fetch('OPENAI_API_KEY'))

class Codegen
  include Anima.new(:openapi_file, :route, :verb, :mime)
  include Memoizable

  def self.parse_cli(argv)
    opts = Slop.parse(argv) do |o|
      o.string '--openapi-file', 'Path to OpenAPI file', required: true
      o.string '--route', 'API route', required: true
      o.string '--verb', 'HTTP verb', required: true
      o.string '--mime', 'Mime type', default: 'application/json'
    end

    openapi_file = Pathname.new(opts[:openapi_file])
    raise ArgumentError, "OpenAPI file #{openapi_file} does not exist" unless openapi_file.exist?

    route = opts[:route]
    verb = opts[:verb]
    mime = opts[:mime]

    new(openapi_file: openapi_file, route: route, verb: verb, mime: mime)
  end

  def validate!
    paths = doc.fetch('paths')
    unless paths.key?(route)

      raise <<~ERR
        Invalid route!

        Given: #{route}

        Valid routes:

        #{paths.keys.sort.join("\n")}
      ERR

    end

    path_def = paths.fetch(route)

    return if path_def.key?(verb)

    raise <<~ERR
      Invalid verb!

      Given: #{verb}

      Valid verbs: #{path_def.keys.sort.join(', ')}
    ERR
  end

  def get?
    verb == 'get'
  end

  def no_request_body?
    !action.key?('requestBody')
  end

  def sample_response
    action.fetch('x-oaiMeta').fetch('response')
  end

  def request_body_summary
    fields = request_body.fetch('properties').keys
    required = request_body.fetch('required')

    { field: fields, required: required }
  end

  def request_body
    ref =
      if mime == 'application/json'
        deep_fetch(action, ['requestBody', 'content', 'application/json', 'schema', '$ref'])
      elsif mime == 'multipart/form-data'
        deep_fetch(action, ['requestBody', 'content', 'multipart/form-data', 'schema', '$ref'])
      else
        raise "Unknown mime type #{mime}"
      end
    get_ref(ref)
  end
  memoize :request_body

  def response_body
    response = action.fetch('responses').first.last
    ref = deep_fetch(response, %w[content application/json schema $ref])
    get_ref(ref)
  end

  def get_ref(ref)
    ref_path = ref.delete_prefix('#/').split('/')
    deep_fetch(doc, ref_path)
  end

  def action
    doc.fetch('paths').fetch(route).fetch(verb)
  end

  def doc
    @doc ||= YAML.load_file(openapi_file)
  end

  def deep_fetch(obj, path)
    path.reduce(obj) do |acc, key|
      acc.fetch(key) do
        raise "No key #{key} in #{acc.inspect}"
      end
    end
  end
end

codegen = Codegen.parse_cli(ARGV)
codegen.validate!

create_completion_example =
  codegen.with(
    route: '/completions',
    verb: 'post',
    mime: 'application/json'
  )

user_message_template = <<~MSG
  Please create an API call, a response wrapper, and a test for this request.
  OpenAPI context in JSON:

  ACTION: %<verb>s %<route>s
  REQUEST MIME TYPE: %<mime>s

  REQUEST SUMMARY: %<summary>s

  SAMPLE RESPONSE: %<response>s
MSG

assistant_response = <<~RUBY
  # api call
  def create_completion(model:, **kwargs)
    Response::Completion.from_json(
      post('/v1/completions', model: model, **kwargs)
    )
  end

  # wrapper
  class Completion < JSONPayload
    class Choice < JSONPayload
      field :text
      field :index
      field :logprobs
      field :finish_reason
    end

    class Usage < JSONPayload
      field :prompt_tokens
      field :completion_tokens
      field :total_tokens
    end

    field :id
    field :object
    field :created
    field :model
    field :choices, wrapper: Choice
    field :usage, wrapper: Usage
  end

  # test
  describe '#create_completion' do
    let(:response_body) do
      {
        "id": 'cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7',
        "object": 'text_completion',
        "created": 1_589_478_378,
        "model": 'text-davinci-003',
        "choices": [
          {
            "text": "\n\nThis is indeed a test",
            "index": 0,
            "logprobs": nil,
            "finish_reason": 'length'
          }
        ],
        "usage": {
          "prompt_tokens": 5,
          "completion_tokens": 7,
          "total_tokens": 12
        }
      }
    end

    let(:response) do
      instance_double(
        HTTP::Response,
        status: HTTP::Response::Status.new(200),
        body: JSON.dump(response_body)
      )
    end

    it 'can create a completion' do
      completion = client.create_completion(model: 'text-davinci-002', prompt: 'Hello, world!')

      expect(http)
        .to have_received(:post)
        .with('https://api.openai.com/v1/completions', hash_including(:json))

      expect(completion.id).to eql('cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7')
      expect(completion.model).to eql('text-davinci-003')
      expect(completion.choices.first.text).to eql("\n\nThis is indeed a test")
      expect(completion.choices.first.index).to eql(0)
      expect(completion.choices.first.logprobs).to be_nil
      expect(completion.choices.first.finish_reason).to eql('length')
      expect(completion.usage.prompt_tokens).to eql(5)
      expect(completion.usage.completion_tokens).to eql(7)
      expect(completion.usage.total_tokens).to eql(12)
    end
  end
RUBY

history = [
  {
    role: 'user',
    content: format(
      user_message_template,
      mime: create_completion_example.mime,
      verb: create_completion_example.verb,
      route: create_completion_example.route,
      summary: create_completion_example.request_body_summary,
      response: create_completion_example.sample_response
    )
  },
  {
    role: 'assistant',
    content: assistant_response
  },
  {
    role: 'user',
    content: format(
      user_message_template,
      mime: codegen.mime,
      verb: codegen.verb,
      route: codegen.route,
      summary: codegen.no_request_body? ? '(none)' : codegen.request_body_summary,
      response: codegen.sample_response
    )
  }
]

cache_dir = Pathname.new(__dir__).parent.join('tmp/codegen')
cache_dir.mkpath unless cache_dir.directory?
cache_file = cache_dir.join("#{codegen.verb}_#{codegen.route.gsub('/', '_')}.txt")

if cache_file.file?
  puts cache_file.read
else
  completion = api.create_chat_completion(
    model: 'gpt-3.5-turbo',
    messages: history,
    max_tokens: 1000,
    temperature: 0
  )
  output = completion.choices[0].message.content
  cache_file.write(output)
  puts output
end
